# Understanding LLM Economics

**Recording:**

{% embed url="https://app.read.ai/analytics/meetings/01HG2BNN2TQR59HN96TFHEYCED?utm_source=Share_CopyLink" %}



**Summary:**

The meeting began with introductions from attendees, who shared their backgrounds and areas of expertise. Nikunj Bajaj provided an overview of his own background and the startup he is currently working for, True Foundry, which is in the ML infraspace. The purpose of the meeting was to discuss the systematic things it takes to make India a place where AI start-ups can thrive. The speaker emphasized the importance of building bridges and growing the ecosystem in India, and explained that the team is trying to figure out what it takes to make India a hub for world-class AI products that can solve problems for everyone.

Nikunj Bajaj presented a comprehensive cost analysis of using LLMs for summarizing Wikipedia. He explained the pricing structure of LLMs and calculated the cost of processing the input and generating the output for different models, including GPT-4, GPT-4 Turbo, GPT 3.5 Turbo, and Anthropic models. He also discussed the trade-offs and benefits of using different models, highlighting the continuous reduction in cost with OpenAI. Additionally, he shared insights on playing around with different open source models, such as Lama 2, Falcon, and 180 billion models.

Nikunj Bajaj provided a detailed explanation of the cost of processing inputs and generating outputs with LLMs, emphasizing that input processing is much faster than output generation due to parallel and sequential processing, respectively. He also discussed the memory cost for fine-tuning and training a model, highlighting that holding a 7 billion parameter model in memory costs around 28 GB of GPU RAM. Nikunj provided a comprehensive comparison of the cost and quality of different language models, including GPT 3.5, GPT-4 Turbo, and Lama 2. He discussed the cost changes and quality aspects of these models, highlighting their strengths and weaknesses in various tasks such as named entity recognition, formatting, and Docs QA.

The meeting also discussed the potential trade-off between helpfulness and safety when using large language models for specific use cases, and how models can be fine-tuned to achieve specific tasks. Nikunj Bajaj provided insights into the use cases and challenges of fine-tuning language models, including the use of DLLM as a marketplace and the limitations of RAG for answering trend questions. The participants shared their use cases and how they justify the costs of their work, emphasizing the importance of collaboration to solve each other's problems. Overall, the meeting focused on the practical aspects of their work and the ways in which they can support each other to achieve their goals.



